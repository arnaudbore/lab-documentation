# UNF SERVERS

This document is intended to explain how the data is organized in the lab on the UNF servers. The first section describes the location and creation of a dataset. A second section includes templates for different types or dataset (RAW data or PREPROCESSED).

## DATASET

### What is a dataset ?

**Dataset** - a set of neuroimaging and behavioral data acquired for a purpose of a particular study. A dataset consists of data acquired from one or more subjects, possibly from multiple sessions.

### Where do datasets live ?

All datasets **MUST** be located here: `/data/simexp/datasets`

### How to create a datasets ?

You can create a dataset using the following commands lines:

```
cd /data/simexp/datasets/
createDatasetStructure NAME_OF_MY_DATASET
```

It will create a folder with this structure:

```
NAME_OF_MY_DATASET
|- CHANGES
|- code/
|- dataset_description.json
|- derivatives/
|- participants.tsv
|- README
|_ sourcedata/
```

### How to manage a dataset ?


## How to create a dataset is defined ?

**Template**. Descriptions of data shared in the SIMEXP lab need to be added to one of the relevant section below, using the following template.

**dataset_label**.	Brief description of the dataset (N=?, types of scans, format - mnc, nii, etc). URL with full description. Paper(s) link. 			
(server name)://(path)

**dataset_label**. This is chosen by each data creator. Try to keep short and explicit. Public data typically already have a name (e.g. hcp, nki, adni, etc). When creating derivatives, make sure you use a name consistent with the one used for raw data!

**Location (path)**. The base location to store data, called (data_location), will vary based on the server. In addition to the base location, various subfolders will be used depending on the nature of the data. In the following sections, we separate conventions based on the fact it is raw data, or generated by a standard pipeline. Different naming conventions exist based on the pipeline, see each relevant section.

| (server_name) | Use case      | location |
| ------------- |:-------------:| -----:|
| elm | High availability, moderate power | $1600 |
| jacaranda | centered      |   $12 |
| meleze | Moderate power (with GPU)      |    $1 |
| sapin | are neat      |    $1 |
| thuya | are neat      |    $1 |
| pin | Moderate power (with GPU) |    $1 |
| ginkgo | GPU server | `/data/simexp/databases` |

Note that some data cannot be shared with the full lab, due to some data usage agreement. Such data may live under some personal accounts (if really necessary), and/or have restricted read access (preferred method, using standard location and documentation). Please try to conform to the standard data organization as much as possible.

Documentation. Please add a README.md file at the root with a detailed description of how the data was generated. Also describe the relevant scripts, and add the scripts at the root. This may not be necessary for pipeline derivatives, as extensive documentation is generated automatically. When you use the raw data to run a preprocessing pipeline, make a not of your name, the date, the pipeline and where the outputs are located so others may find these data rather than re-run their own preprocessing.

Data freezing. Once a dataset is generated and QCed, either through download or running a pipeline, change the permissions to read only
chmod u-w -R (path)
chmod g-w -R (path)
Do not apply any change to the folder in the future.
If you absolutely have to change a dataset, for example to fix an error, document the change and date in the README.md

Symlinks. RAW and PREPROCESSED folders are meant to be read only. If you want to reorganize data or combine data from several datasets in a convenient location, you can use symbolic links. A symbolic link looks and behaves like a file in a folder but isn’t a copy of the original file. Rather, the operating system knows where the original file is located and links your “symbolic” file to this location. This has several benefits: 1) You can make as many symbolic links to an original dataset as you like without wasting space 2) You can always be sure that your “copy” of the data is up to date and hasn’t changed. 3) You can rename the symbolic link to whatever you please without changing the name of the original file. 4) Symbolic links are extremely fast to create and remove, so you’ll save a lot of time compared to copying data.

To create a symbolic link like this:
ln -s /path/to/original/file_or_folder /path/where/symlink/should/go

You can check if a file is an “original” or a “symbolic” file by typing
ls -l

This is what a symbolic link looks like (the path of the original is after the ->)
file.txt -> local_projects/distant_original_file.txt

This is what an original file looks like (it doesn’t have the -> if you type ls -l)
original_file.txt

To remove a symbolic link do this:
	unlink /path/where/symlink/should/go

Using unlink is a better habit of removing symbolic links compared to using rm because rm can in certain cases remove the original file (if you have write permission) while unlink will never do that.

Communication. Any data activity at the SIMEXP lab needs to be announced on the #data simexp channel, with a @channel mention. This includes release of a new data, archival of data, cloning of data on a new server, or patch of an existing data. Discussions leading to a data activity should also be posted publicly on #data, possibly mentioning users know to be involved with this resource. We advise you join and monitor #data, because some people may not be aware of your use of certain resources, and implement changes that affect you without mentioning you in a discussion.

Image format. Do not mix mnc and nifti in the same path. Duplicate data if needed. See niak_brick_mnc2nii and niak_brick_nii2mnc. Also, some imaging files come with an _extra.mat_ companion file. This file contains critical information on timing, and needs to be copied and correctly interpreted. This will be automatic if you are using niak. From python, avoid doing it altogether, and start from minimally preprocessed data, which do not come with an _extra.mat file.

Choice of the server (server_name). Different servers have different characteristics, summarized in the table above.
 By default, all key results need to be copied at some point on stark. A backup of stark data is automatically generated every day, with two off-site copies.
Data stored on other production servers are there only temporarily, for analysis.
 If a dataset exists on multiple servers, make sure the two versions are synced at all times using ‘rsync -av  source destination’.
If a dataset is not currently active in any research project, it needs to be archived. Archiving, or retrieving from archive, is fast. Procedure for data archival will be shared soon.





When you are new to Compute Canada, we will ask you to

- delete the default folders that Compute Canada creates for each user in the shared storage spaces on each cluster  that you are using
  `for d in /{project,nearline}/*-pbellec/$USER ; do rmdir $d && ln -s /dev/null $d ; done`

When you download a dataset:

1. Contact the lab's data admins, Basile and Arnaud, prior to downloading the data, and ask how to best proceed if uncertain.
1. Please use BIDS storage if available, and use datalad to "install" the dataset
2. Data should be store in `~/project/(rrg|def)-pbellec/DATA`
4. Create a folder for the dataset in  `~/project/(rrg|def)-pbellec/DATA/new_dataset/derivatives`
3. To set the access rights there are two possible cases:

  5.1. For datasets that requires the applicants to sign individual non-sharing agreement.

  When applying, it is important that you involve the lab's data admins so they can also sign the non-sharing agreement or other nescessary documents.

  Set read access rights through ACLs to all your colleagues who have signed the agreement, and will be using the data. Be sure to also give read access rights to the lab's data admins.

  ```
  setfacl -R u:colleague1:r u:colleague2:r .... u:admin1:r u:admin2:r ... ~/projects/rrg-pbellec/DATA/new_dataset
  ```

  If the other colleagues will contribute to the analysis, give them write access to the derivatives folders.

  ```
  setfacl -R u:colleague1:rw u:colleague2:rw ~/projects/rrg-pbellec/DATA/new_dataset/derivatives
  ```

 5.2 For all other types of datasets.

For datasets that do not require individuals to sign non-sharing agreeements you can give read access to anyone. This means fellow lab members who are interested in using the same dataset can do so without re-dowloading/duplicating/wasting storage.

  ```
  chmod -R g+r ~/projects/rrg-pbellec/DATA/new_dataset
  ```
  And give write access to the derivatives
  ```
  chmod -R g+rw ~/projects/rrg-pbellec/DATA/new_dataset/derivatives
  ```

 4. When you run preprocessing, analysis, .... store the outputs in the derivatives folder, with an explicit naming, and a version of the software used, eg.

```
~/projects/rrg-pbellec/DATA/new_dataset/derivatives/fmriprep-20.0.4
~/projects/rrg-pbellec/DATA/new_dataset/derivatives/my_fancy_new_analysis-0.0.2
```
  Same as above, you must, at a minimim, give read access to the data admins.

When you install a new software, yours, or others, install it in the `SRC` folder.



Our different types of dataset

Raw_data

This type of data typically lives under
stark://data/cisl/raw_data/(dataset_label)/raw_mnc
or	stark://data/cisl/raw_data/(dataset_label)/raw_nii
Depending on the imaging file format. If multiple releases are available, this can be appended to the folder name, e.g. ‘raw_nii_release1’. Make sure to document where each release comes from in the README.md. Undocumented data will be deleted, as it is essentially useless.

Midnight		The midnight brain scan project https://openfmri.org/dataset/ds000224/

stark://data/cisl/raw_data/midnight/Rawdata
				Date: ??

        Preprocessed_data

        This type of data is typically generated by niak_pipeline_fmri_preprocess or fmri_prep, called (pipeline_name). It typically lives under
        (server_name)://(base_location)/(dataset_label)/(pipeline_name)

         If multiple preprocessing are available, this can be appended to the folder name, e.g. ‘(dataset_label)/niak_fmri_preprocess_cog-1.0.3’.

        No need to write a README.md, the pipeline generates automatically extensive provenance tracking information. But do copy the script used to preprocess the data at the root of the folder.
